{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Execution modes\n",
    "\n",
    "1. Interactive Client (Spark-shell, notebooks) Used for exploration\n",
    "2. Submit Job (spark-submit, Databricks, RestAPI) In production cluster\n",
    "\n",
    "### Processing Model\n",
    "Aplies a master-slave process. When we submit our application to spark, it would create a master(Driver) process. And this master process creates slave(Executor) process. The driver and executors are assigned by Cluster manager (YARN).\n",
    "\n",
    "**Spark Cluster Manager** \n",
    "1. local[n] (n refers multi threading) if n=3, driver=1 and executor =2 \n",
    "2. YARN\n",
    "3. Kubernetes\n",
    "4. Mesos\n",
    "5. Standalone\n",
    "\n",
    "**-----------------------------------------------------------------------------------------------------------**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Programming model\n",
    "\n",
    "**Spark Session** (is the driver)\n",
    "- when we launch a shell, usually the sessions are created and are available as 'spark'\n",
    "- But when writing program, we need manually write it\n",
    "\n",
    "```\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Hello word\")\\\n",
    "        .master(\"local[3]\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "We can configure Spark Sessions via:\n",
    "1. Environment variables (precendence 4)\n",
    "2. SPARK_HOME\\conf\\spark-defaults.conf (precendence 3)\n",
    "3. spark-submit command-line options (precendence 2)\n",
    "4. SparkConf Object (Applocation code) (precendence 1)\n",
    "\n",
    "**Spark DataFrame**\n",
    "\n",
    "1. Read \n",
    "```\n",
    "spark_df = spark.read\n",
    "                .option(\"header\",\"true\")\n",
    "                .option(\"inferSchema\",\"true\")\n",
    "                .csv(sys.args[1])\n",
    "```\n",
    "\n",
    "**Spark DataFrame Partitions**\n",
    "- logical small in memory dataframe. The data is partitioned on your HDFS.\n",
    "- the partitions loading into the executors are handles by Resource Manager.\n",
    "\n",
    "**Spark Transformation**\n",
    "- **Spark dataframes are immutable**\n",
    "\n",
    "```\n",
    "filtered_df = survey_df.where(\"Age\" < 40).select(\"Name\",\"age\").groupby(\"Country\")\n",
    "\n",
    "filtered_df.collect()\n",
    "\n",
    "```\n",
    "\n",
    "**Transformation**\n",
    "1. Narrow dependencies - Transformation performed independently on a single partition to produce valid results. (Example: where)\n",
    "2. Wide dependency - A transformation that requires data from other partitions to produce valid results. (Example: group by). This performs Shuffle and sort exchange which is internally managed.\n",
    "\n",
    "**Lazy evaluation**\n",
    "Transformations are lazy\n",
    "\n",
    "**Spark Action**\n",
    "read, write, collect, show are examples. This triggers the spark execution and are not lazy. Actions are converted to *jobs*.\n",
    "\n",
    "### Understanding the execution plan\n",
    "\n",
    "App code --> Jobs --> stages --> tasks \n",
    "\n",
    "**-----------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark API\n",
    "\n",
    "RDD APIS (core) --> catalyst Optimizer --> Spark SQL, Dataframe API, Dataset API (scala or java)\n",
    "\n",
    "**Spark RDD**\n",
    "Resilient distributed dataset. \n",
    "R -- fault tolerant\n",
    "They lack a row column structure.\n",
    "\n",
    "If you want spark RDD, then you need to use sparkContext\n",
    "```\n",
    "conf = SparkConf()\\\n",
    "        .setMaster(\"local[3]\") \\\n",
    "        .setAppName(\"Hello\")\n",
    "\n",
    "--Option 1--\n",
    "sc= SparkContext(conf=conf)\n",
    "\n",
    "--Option 2--\n",
    "spark = SparkSession \\\n",
    "        .builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "sc = spark.sparkContext \n",
    "\n",
    "linesRDD = sc.testFile(sys.argv[1])\n",
    "\n",
    "```\n",
    "\n",
    "Spark SQL engine\\Catalyst optimizer\n",
    "1. Analysis\n",
    "2. Logical optimization\n",
    "3. Physical planning (set of RDD operation)\n",
    "4. Code generation (generate efficient byte code)\n",
    "\n",
    "**-----------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark data sources and sinks\n",
    "\n",
    "DataFrameReader API\n",
    "\n",
    "Example:\n",
    "```\n",
    "spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"heaader\",\"true\")\n",
    "    .option(\"path\",\"/data/mycsvfiles/\")\n",
    "    .option(\"mode\",\"FAILFAST\")\n",
    "    .schema(mySchema)\n",
    "    .load()\n",
    "```\n",
    "\n",
    "Read Mode\n",
    "1. PERMISSIVE  -- all fields are null when encounters a corrupted the record\n",
    "2. DROPMALFORMED -- drops\n",
    "3. FAILFAST -- raises exception\n",
    "\n",
    "Schema inference doesn't work well with csv and JSON.\n",
    "*Parquet* comes with **in built schema info**\n",
    "\n",
    "\n",
    "Spark schema definition\n",
    "1. Programmatically\n",
    "2. Using DDL scripts (Column datatype seperated by,)\n",
    "\n",
    "DataFrameWriter API\n",
    "\n",
    "```\n",
    "DataFrame.write\n",
    "        .format(\"parquet\")\n",
    "        .mode(SaveMode) --given below\n",
    "        .option(\"path\", \"/data/flights/\")\n",
    "        .save()\n",
    "```\n",
    "\n",
    "**Save Modes**\n",
    "1. append\n",
    "2. overwrite\n",
    "3. errorIfExists\n",
    "4. ignore\n",
    "\n",
    "Spark File Layout\n",
    "1. Number of files and file size\n",
    "2. Partitions and Buckets\n",
    "3. Sorted data  \n",
    "\n",
    "\n",
    "- Number of files created depends upon dataframe partitions\n",
    "- `MaxRecordsPerFile` is used controls the number of rows or file size\n",
    "\n",
    "We can create database\n",
    "1. Tables\n",
    "2. Views\n",
    "\n",
    "Spark allows us to create two sets of tables Managed tables(stored inside the warehouse directory) and unmanaged tables (External table)\n",
    "\n",
    "```\n",
    "Add .enableHiveSupport() to spark session\n",
    "\n",
    "spark.catalog.setCurrentDatabase(\"AIRLINE_DB\")\n",
    "filghtdf.write.mode(\"overwrite\").saveAsTable(\"flight_data_tble\")\n",
    "```\n",
    "**-----------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data transformation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
