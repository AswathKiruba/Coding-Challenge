{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "\n",
    "It contains\n",
    "1. HDFS: File system to manage the storage of data\n",
    "2. MapReduce: A framework to process data across multiple servers\n",
    "\n",
    "* Based on GFS\n",
    "\n",
    "In 2013, Hadoop's MapReduce was divided into 2\n",
    "1. MapReduce (defines): framework to define a data processing task\n",
    "2. YARN (runs): framework to run the data processing tasks\n",
    "\n",
    "# Intro\n",
    "\n",
    "### HDFS\n",
    "Hadoop is deployed on a group of machine (refered as cluster). Each machine in the cluster is called as node.\n",
    "* **Name node**: acts as mster node. manages overall file system.\n",
    "    * It stores the directory structure\n",
    "    * Metadata for all the files (block locations of each file)\n",
    "* **Data node**: data is physically stored in name nodes\n",
    "\n",
    "For example a file is broken into blocks of size 128 MB.\n",
    "**How is the size choosed?**\n",
    "Aimed to minimize the time to seek the block on the disk. Once seeked we could read the data. \n",
    "\n",
    "**Steps on reading a File**\n",
    "\n",
    "1. The metadata in name node (its referred first)\n",
    "2. The blocks in the data node\n",
    "\n",
    "**What if one of the blocks get corrupted or what happens when a node fail?**\n",
    "Solution: replication factor. Copy the blocks in different node.\n",
    "\n",
    "### MapReduce\n",
    "\n",
    "**Find the number of time hello has occured in the file**\n",
    "\n",
    "1. **Map Phase**: Process each block in the node that its stored and get intermediate results.\n",
    "2. **Reduce Phase**: Take all the results to one node and combine them.\n",
    "\n",
    "### YARN\n",
    "\n",
    "**Yet another resource negotiator**\n",
    "1. Resposible for the management of resources on the Hadoop cluster.\n",
    "2. Monitor failures\n",
    "\n",
    "### Different modes\n",
    "1. Standalone mode: only one node (technically no HDFS, no YARN)\n",
    "2. Pseudo-distributed: two nodes are running,one acts as a name node and other as a data node\n",
    "3. Fully distributed: Manually configuring and managing nodes in a Hadoop cluster is complicated and error-prone. We could use enterprise edition of Hadoop: Cloudera, Hortonworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "Any data processing task can be parallelized if we express it as \n",
    "<key, value> --> map() --> <key, value> --> reduce()\n",
    "\n",
    "We could also chain transformations.\n",
    "\n",
    "**Once the results from all the mapper come, it stored on a single node. In this node an operation called sort/merge occurs.**\n",
    "\n",
    "## Parallelize your reduce operation\n",
    "\n",
    "### Combiner function\n",
    "(reducer applied at each data node)\n",
    "\n",
    "Map operation is highly parallelized. Example: constitutes to the number of data nodes which actually contains the blocks of data.\n",
    "\n",
    "Instead of sending the mapper results to a single node to perform sort/merge we could use the combiner function to merge/combine the key value pairs with the same keys.\n",
    "\n",
    "The combiner function is implemented by extending the reducer class.\n",
    "\n",
    "**IMP**\n",
    "The combiner makes the MapReduce task faster by \n",
    "1. Reduce operation is partially parallelized\n",
    "2. Less data needs to be transferred across the network\n",
    "\n",
    "### Constraints on combiner function\n",
    "\n",
    "NOTE: The combiner function should have no impact on the final result. In fact, the decision of whether to use the combiner or not ultimately rests with Hadoop.\n",
    "For example: Average is a reducer function, if we use it as a combiner function we might end up getting wrong results.\n",
    "```\n",
    "Data node 1 --> MAP --> \n",
    "Data node 1 --> MAP --> Sort/merge --> Reduce\n",
    "Data node 1 --> MAP -->\n",
    "\n",
    "```\n",
    "\n",
    "Average of a set of number <> Average (Average of subset of numbers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Sort\n",
    "\n",
    "(Allows us to use multiple reducer)\n",
    "\n",
    "**How is a map task excuted?**\n",
    "Is executed by a \"process\" on a node in the cluster. A process is a job which has its own memory and manages its own resources.\n",
    "\n",
    "All the processes are launched and managed by YARN.\n",
    "- Map processes are called as mappers\n",
    "- Reduce processes are called as reducers\n",
    "\n",
    "**How many mappers and reducers does a MapReduce job require?**\n",
    "\n",
    "**Mappers**\n",
    "- The number of mappers are usually controlled by YARN. Each mapper operates on one HDFS block (128 MB). The number of mappers depends on the **number of splits** i.e., the number of blocks your input file is broken into. This is number which YARN uses.\n",
    "- This effectively means that multiple mappers can be assigned to 1 data node. (This is in reality)\n",
    "- The number of mappers can only be controlled to a limited extent by the user.\n",
    "\n",
    "**Reducers**\n",
    "- The number of reducers can be controlled by users\n",
    "- By default, there is only 1 reducer (which is what we have seen so far)\n",
    "- With multiple reducers, the reduce task can also be parallelized\n",
    "\n",
    "The number of reducers can be specified when we run the job.\n",
    "\n",
    "**Scenario**\n",
    "- Let's say our input file has been split into 4 blocks by HDFS.There would be 4 mappers.\n",
    "<img src=\"./Image/image1.png\">\n",
    "\n",
    "When there are 2 reducers, the output of each map is first partitioned into 2 partitions. \n",
    "\n",
    "**How do we partition our map output?**\n",
    "The map output is assigned to a partition based on a key. All the values with the same key must be assigned to the same partition.\n",
    "\n",
    "The output of partition 1 --shuffled and sort--> reducer1 and partition 2 --shuffled and sor--> reducer 2\n",
    "\n",
    "Partitioning and Shuffle/Sort are 2 extra operations that happens for MR jobs with multiple reducers.\n",
    "\n",
    "<img src=\"./Image/image2.png\">\n",
    "\n",
    "\n",
    "`Number of partitions = Number of reducers`\n",
    "\n",
    "**Hadoop taskes care of mapping keys to partition**\n",
    "The partition number of key depends on it hash value.\n",
    "Partition logic should be such that the keys are evenly distributed among the partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Streaming API\n",
    "\n",
    "We can implement the map() and reduce() functions in ANY LANGUAGE. The streaming API uses standard INP/OUT inorder to communicate with our program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS\n",
    "\n",
    "Hadoop is normally deployed on a group of machines. This group of machines is called as cluster. Every machine in the cluster is a node.\n",
    "\n",
    "**Name Node**\n",
    "One of the node acts as the master node, which manages the overall file system. It store\n",
    "1. Directory structure\n",
    "2. Metadata for all the files\n",
    "\n",
    "**Data Nodes**\n",
    "Other nodes are called data nodes. The data is physically stored on these nodes. \n",
    "\n",
    "**Scenario: Files stored in HDFS**\n",
    "- First the file is broken into blocks of size 128 MB\n",
    "(The size is chosen to minimize the time to seek to the block on the disk)\n",
    "- These blocks are then stored accross data nodes. A data node can contains more than one blocks\n",
    "\n",
    "**What if one of the block or node gets crashed?**\n",
    "Define replication factor in HDFS. It is defined in configuration file `hdfs-site.xml`.\n",
    "\n",
    "In prod usually the replication factor is 3.\n",
    "\n",
    "**The replica locations are chosen to maximize redundancy while minimizing write bandwidth.**\n",
    "- To **maximize redundancy** all replicas should be sored \"far away\" i.e., different racks.\n",
    "\n",
    "* **minimize the write bandwidth**\n",
    "    * When a file is written to HDFS, a single node runs the replication pipeline. \n",
    "    * A node is chosen and this node forwards the data to the location chosen for the second replica. (First within the rack then across the rack).\n",
    "    * All this consumes network bandwidth. More the number of racks to traverse, more the bandwidth consumed.\n",
    "    \n",
    "### Default Strategy\n",
    "For a replication factor of 3\n",
    "1. First replica is chosen in the same rack but different node\n",
    "2. Second replica is chosen in a different rack (rack 2) and stored in node 1\n",
    "3. Third replica is chosen in rack 2 but different node 2\n",
    "\n",
    "\n",
    "### Why Name Node is critical?\n",
    "\n",
    "Hadoop is normally deployed on a group of machines (class as cluster). Each machine is called as  a node. One node is called as the name node. \n",
    "Name node stores:\n",
    "1. directory structure\n",
    "2. Metadata for all the files\n",
    "\n",
    "- It also stores replica info (whether master or replica)\n",
    "- Stores all metadata of files such as name, owner, permissions etc.\n",
    "\n",
    "**NOTE**\n",
    "- Persistent: File metadata, directory structure etc are persistently stored on the Name node (on disk)\n",
    "- Non persistent: block loations are stored in memory\n",
    "\n",
    "if name node fails, all the files are lost\n",
    "\n",
    "Hadoop makes the name node failure resistant in 2 ways\n",
    "**Step 1:** back up all the files that store file system metadata\n",
    "- There are 2 files that store the file system metadata. 1.fsimage 2.edits (together this represent the state of the file system at start uo)\n",
    "- fsimage is a snapshot that is loaded into memory. All the changes to file system are then made in-memory\n",
    "- edits file stores a lof of all in-memory edits to the filesystem\n",
    "\n",
    "You can use them to reconstruct the name node. They are usually backed up on local filesystem of name node.\n",
    "Cons: it compute heavy to reconstruct the name node using these files\n",
    "\n",
    "**Step 2:** Maintain a seconday name node\n",
    "- Secondary name node maintains a merged fsimage\n",
    "- It periodically merges the fsimage and edits files and copies it to the secondary name node.\n",
    "- This process is called as checkpointing\n",
    "\n",
    "The secondary name node can take in place of the primary name node.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YARN\n",
    "\n",
    "#place holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop as database\n",
    "\n",
    "### Querying data using MR\n",
    "\n",
    "- You can store data in a structured format eg (csv, xml, json)\n",
    "- Hadoop will not enforce any constaints on these data (eg: primary key)\n",
    "\n",
    "Why hadoop?\n",
    "- processing is parallelized, and we could scale linearly (This means that all it take to double the performance is to double the number of nodes)\n",
    "\n",
    "In a traditional databasse we need to do different things to improve read performance vs write performance. These 2 are usually in counterbalance. Indexing improve reads, but keeping the index updated will affect the  write performance. Not just that, an index requires disk space and the cost of disk space increases in a non-linear fashion.\n",
    "\n",
    "### SELECT FROM & WHERE\n",
    "\n",
    "- trading data in csv structure\n",
    "- Before selecting MR should satisfy the WHERE\n",
    "- Mapper discards the rows where conditions doesn't satisfy\n",
    "\n",
    "   <Rownum, Input Row> ---> MAP --> <RowNum , Output Row> (Output Row contains the columns in SELECT) --> Reducer <Null, Output Row>\n",
    "   \n",
    "- Reducer discards the RowNum and just writes out each row as it comes to it.\n",
    "\n",
    "### GROUP BY AND HAVING\n",
    "\n",
    "```\n",
    "SELECT symbol, MAX(OPEN)\n",
    "WHERE series = 'EQ'\n",
    "GROUP BY symbol\n",
    "HAVING MAX(OPEN)>20\n",
    "```\n",
    " \n",
    "**MAPPER** \n",
    "- Mapper discards the rows where conditions doesn't satisfy\n",
    "- Once WHERE is satisfied, then a TextObject is constructed using the columns in SELECT\n",
    "\n",
    "    <RownNum, InputRow> --> MAP --> <GroupByKey, other columns>\n",
    " \n",
    "- Grouping is an operation that is natural part of MapReduce\n",
    "\n",
    "**REDUCER**\n",
    "\n",
    "    <GroupByKey, other columns> --> REDUCER --> <GROUPBYKEY, AGG column>\n",
    "    \n",
    "### MapReduce Join - The MAP side\n",
    "\n",
    "- Each join needs to be handled differently\n",
    "- Use different mapper for each table\n",
    "    \n",
    "    <RowNum, <Row, Tablename>> --> MAP --> <Joinkeycolumn, <other columns, tablename>>\n",
    "    \n",
    "    Table 1:\n",
    "    \n",
    "    N.symbol, N.name (RIL, Reliance)  --> RIL, \"Reliance\",N\n",
    "    \n",
    "    Table 2:\n",
    "    \n",
    "    T.symbol, T.High, T.Timestamp  --> RIL, \"100,02DEC2014\",T\n",
    "    \n",
    "    \n",
    "- Reducer should be able to combine records with same column value\n",
    "    \n",
    "### MapReduce Join - The Reduce side\n",
    "\n",
    "This is only for outer join\n",
    "\n",
    "- Input to reducer is from both tath\n",
    "- For every symbol the reducer gets a list of values of type <Other, Columns, Table>\n",
    "\n",
    "It would be more efficient if the first record the reducer sees it the one from Names (has distinct row). This done by seconday sort.\n",
    "\n",
    "- Reducer is where the actual joins happen.\n",
    "\n",
    "<Joinkeycolumn, <other columns, tablename>> --> REDUCER --> <<joinkey, name>, <other column, other column>>\n",
    "\n",
    "TO use secondary sort, input to reducer should be changed\n",
    "\n",
    "<<join key, table> other columns> --> REDUCER --> <<joinkey, name>, <other column, other column>>\n",
    "\n",
    "Sorting would be easier if table name is integer.\n",
    "\n",
    "**NOTE:** sorting adds efficiency only if one of the tables is joined on a unique key.\n",
    "\n",
    "\n",
    "### Sorting and Partitioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
