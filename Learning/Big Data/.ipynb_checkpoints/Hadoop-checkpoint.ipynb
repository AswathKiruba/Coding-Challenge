{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "\n",
    "It contains\n",
    "1. HDFS: File system to manage the storage of data\n",
    "2. MapReduce: A framework to process data across multiple servers\n",
    "\n",
    "* Based on GFS\n",
    "\n",
    "In 2013, Hadoop's MapReduce was divided into 2\n",
    "1. MapReduce (defines): framework to define a data processing task\n",
    "2. YARN (runs): framework to run the data processing tasks\n",
    "\n",
    "# Intro\n",
    "\n",
    "### HDFS\n",
    "Hadoop is deployed on a group of machine (refered as cluster). Each machine in the cluster is called as node.\n",
    "* **Name node**: acts as mster node. manages overall file system.\n",
    "    * It stores the directory structure\n",
    "    * Metadata for all the files (block locations of each file)\n",
    "* **Data node**: data is physically stored in name nodes\n",
    "\n",
    "For example a file is broken into blocks of size 128 MB.\n",
    "**How is the size choosed?**\n",
    "Aimed to minimize the time to seek the block on the disk. Once seeked we could read the data. \n",
    "\n",
    "**Steps on reading a File**\n",
    "\n",
    "1. The metadata in name node (its referred first)\n",
    "2. The blocks in the data node\n",
    "\n",
    "**What if one of the blocks get corrupted or what happens when a node fail?**\n",
    "Solution: replication factor. Copy the blocks in different node.\n",
    "\n",
    "### MapReduce\n",
    "\n",
    "**Find the number of time hello has occured in the file**\n",
    "\n",
    "1. **Map Phase**: Process each block in the node that its stored and get intermediate results.\n",
    "2. **Reduce Phase**: Take all the results to one node and combine them.\n",
    "\n",
    "### YARN\n",
    "\n",
    "**Yet another resource negotiator**\n",
    "1. Resposible for the management of resources on the Hadoop cluster.\n",
    "2. Monitor failures\n",
    "\n",
    "### Different modes\n",
    "1. Standalone mode: only one node (technically no HDFS, no YARN)\n",
    "2. Pseudo-distributed: two nodes are running,one acts as a name node and other as a data node\n",
    "3. Fully distributed: Manually configuring and managing nodes in a Hadoop cluster is complicated and error-prone. We could use enterprise edition of Hadoop: Cloudera, Hortonworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "Any data processing task can be parallelized if we express it as \n",
    "<key, value> --> map() --> <key, value> --> reduce()\n",
    "\n",
    "We could also chain transformations.\n",
    "\n",
    "**Once the results from all the mapper come, it stored on a single node. In this node an operation called sort/merge occurs.**\n",
    "\n",
    "## Parallelize your reduce operation\n",
    "\n",
    "### Combiner function\n",
    "(reducer applied at each data node)\n",
    "\n",
    "Map operation is highly parallelized. Example: constitutes to the number of data nodes which actually contains the blocks of data.\n",
    "\n",
    "Instead of sending the mapper results to a single node to perform sort/merge we could use the combiner function to merge/combine the key value pairs with the same keys.\n",
    "\n",
    "The combiner function is implemented by extending the reducer class.\n",
    "\n",
    "**IMP**\n",
    "The combiner makes the MapReduce task faster by \n",
    "1. Reduce operation is partially parallelized\n",
    "2. Less data needs to be transferred across the network\n",
    "\n",
    "### Constraints on combiner function\n",
    "\n",
    "NOTE: The combiner function should have no impact on the final result. In fact, the decision of whether to use the combiner or not ultimately rests with Hadoop.\n",
    "For example: Average is a reducer function, if we use it as a combiner function we might end up getting wrong results.\n",
    "```\n",
    "Data node 1 --> MAP --> \n",
    "Data node 1 --> MAP --> Sort/merge --> Reduce\n",
    "Data node 1 --> MAP -->\n",
    "\n",
    "```\n",
    "\n",
    "Average of a set of number <> Average (Average of subset of numbers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Sort\n",
    "\n",
    "(Allows us to use multiple reducer)\n",
    "\n",
    "**How is a map task excuted?**\n",
    "Is executed by a \"process\" on a node in the cluster. A process is a job which has its own memory and manages its own resources.\n",
    "\n",
    "All the processes are launched and managed by YARN.\n",
    "- Map processes are called as mappers\n",
    "- Reduce processes are called as reducers\n",
    "\n",
    "**How many mappers and reducers does a MapReduce job require?**\n",
    "\n",
    "**Mappers**\n",
    "- The number of mappers are usually controlled by YARN. Each mapper operates on one HDFS block (128 MB). The number of mappers depends on the **number of splits** i.e., the number of blocks your input file is broken into. This is number which YARN uses.\n",
    "- This effectively means that multiple mappers can be assigned to 1 data node. (This is in reality)\n",
    "- The number of mappers can only be controlled to a limited extent by the user.\n",
    "\n",
    "**Reducers**\n",
    "- The number of reducers can be controlled by users\n",
    "- By default, there is only 1 reducer (which is what we have seen so far)\n",
    "- With multiple reducers, the reduce task can also be parallelized\n",
    "\n",
    "The number of reducers can be specified when we run the job.\n",
    "\n",
    "**Scenario**\n",
    "- Let's say our input file has been split into 4 blocks by HDFS.There would be 4 mappers.\n",
    "<img src=\"./Image/image1.png\">\n",
    "\n",
    "When there are 2 reducers, the output of each map is first partitioned into 2 partitions. \n",
    "\n",
    "**How do we partition our map output?**\n",
    "The map output is assigned to a partition based on a key. All the values with the same key must be assigned to the same partition.\n",
    "\n",
    "The output of partition 1 --shuffled and sort--> reducer1 and partition 2 --shuffled and sor--> reducer 2\n",
    "\n",
    "Partitioning and Shuffle/Sort are 2 extra operations that happens for MR jobs with multiple reducers.\n",
    "\n",
    "<img src=\"./Image/image2.png\">\n",
    "\n",
    "\n",
    "`Number of partitions = Number of reducers`\n",
    "\n",
    "**Hadoop taskes care of mapping keys to partition**\n",
    "The partition number of key depends on it hash value.\n",
    "Partition logic should be such that the keys are evenly distributed among the partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Streaming API\n",
    "\n",
    "We can implement the map() and reduce() functions in ANY LANGUAGE. The streaming API uses standard INP/OUT inorder to communicate with our program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS\n",
    "\n",
    "Hadoop is normally deployed on a group of machines. This group of machines is called as cluster. Every machine in the cluster is a node.\n",
    "\n",
    "**Name Node**\n",
    "One of the node acts as the master node, which manages the overall file system. It store\n",
    "1. Directory structure\n",
    "2. Metadata for all the files\n",
    "\n",
    "**Data Nodes**\n",
    "Other nodes are called data nodes. The data is physically stored on these nodes. \n",
    "\n",
    "**Scenario: Files stored in HDFS**\n",
    "- First the file is broken into blocks of size 128 MB\n",
    "(The size is chosen to minimize the time to seek to the block on the disk)\n",
    "- These blocks are then stored accross data nodes. A data node can contains more than one blocks\n",
    "\n",
    "**What if one of the block or node gets crashed?**\n",
    "Define replication factor in HDFS. It is defined in configuration file `hdfs-site.xml`.\n",
    "\n",
    "In prod usually the replication factor is 3.\n",
    "\n",
    "**The replica locations are chosen to maximize redundancy while minimizing write bandwidth.**\n",
    "- To **maximize redundancy** all replicas should be sored \"far away\" i.e., different racks.\n",
    "\n",
    "* **minimize the write bandwidth**\n",
    "    * When a file is written to HDFS, a single node runs the replication pipeline. \n",
    "    * A node is chosen and this node forwards the data to the location chosen for the second replica. (First within the rack then across the rack).\n",
    "    * All this consumes network bandwidth. More the number of racks to traverse, more the bandwidth consumed.\n",
    "    \n",
    "### Default Strategy\n",
    "For a replication factor of 3\n",
    "1. First replica is chosen in the same rack but different node\n",
    "2. Second replica is chosen in a different rack (rack 2) and stored in node 1\n",
    "3. Third replica is chosen in rack 2 but different node 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
